<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>SCFlow2: Plug-and-Play Object Pose Refiner with Shape-Constraint Scene Flow</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.onload = function() {
      var videos = document.getElementsByTagName('video');
      for (var i = 0; i < videos.length; i++) {
        videos[i].setAttribute('preload', 'auto');
      }
    }
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="./static/images/favicon.svg"> -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">SCFlow2: Plug-and-Play Object Pose Refiner with Shape-Constraint Scene Flow</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://scholar.google.com.hk/citations?user=289PvugAAAAJ&hl=zh-CN">Qingyuan Wang</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=_SKooBYAAAAJ&hl=zh-CN">Rui Song</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?hl=zh-CN&user=Ccu3-acAAAAJ&view_op=list_works&sortby=pubdate">Jiaojiao Li</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=hpt1ehgAAAAJ&hl=zh-CN">Kerui Cheng</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=kW1QrJYAAAAJ&hl=en">David Ferstl</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=dhmdaoQAAAAJ&hl=en">Yinlin Hu</a><sup>3</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>State Key Laboratory of ISN, Xidian University</span>
            <span class="author-block"><sup>2</sup>Taiyuan University of Technology</span>
            <span class="author-block"><sup>3</sup>MagicLeap</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/xxxx.xxxx"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/xxxx.xxxx"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>

              <span class="link-block">
                <a href="https://github.com/W-QY/SCFlow2"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">Nerfies</span> turns selfie videos from your phone into
        free-viewpoint
        portraits.
      </h2>
    </div>
  </div>
</section> -->




<section class="section">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="image-container">
        <img src="./static/images/fig3.png" alt="Object pose refinement">
      </div>
    </div>
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We introduce SCFlow2, a plug-and-play refinement framework for 6D object pose estimation. 
            Most recent 6D object pose methods rely on refinement to get accurate results. However, most 
            existing refinements either suffer from noises in establishing correspondences, or rely on 
            retraining for novel objects. SCFlow2 is based on the SCFlow model designed for iterative RGB 
            refinement with shape constraint, but formulates the additional depth as a regularization in 
            the iteration via 3D scene flow for RGBD frames. The key design of SCFlow2 is an introduction 
            of geometry constraints into the training of recurrent match network, by combining the rigid-motion 
            embeddings in 3D scene flow and 3D shape prior of the target. We train the refinement network on a 
            combination of dataset Objaverse, GSO and ShapeNet, and demonstrate on BOP datasets with novel objects 
            that, after using our method, the result of most state-of-the-art methods improves significantly, 
            without any retraining or fine-tuning.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->




    <div class="hero-body">
      <div class="image-container">
        <img src="./static/images/fig1_s.png" alt="Object pose refinement">
      </div>
      <div class="content has-text-justified">
        <p>
          <b>Object pose refinement</b> is critical for accurate object pose estimation.  
          <b>(a)</b> Most existing object pose refinement methods, including <a href="https://github.com/YangHai-1218/SCFlow">SCFlow</a>, rely on retraining for novel objects to achieve high accuracy.  
          <b>(b)</b> The proposed SCFlow2 achieves even higher accuracy, and more importantly, generalizes well to novel objects without any retraining or fine-tuning.
        </p>
      </div>
    </div>
    
    <div class="hero-body">
      <div class="image-container">
        <img src="./static/images/fig2.png" alt="SCFlow & SCFlow2">
      </div>
      <div class="content has-text-justified">
        <p>
          <b>Design overview of SCFlow and SCFlow2.</b> Given the object 3D mesh, we render an image <i>I<sub>1</sub></i> and depth map <i>D<sub>1</sub></i> based on an initial pose, and then use networks to compare these rendered outputs with the real input <i>I<sub>2</sub></i> and <i>D<sub>2</sub></i> to refine the pose.
          <b>(a)</b> Although <a href="https://github.com/YangHai-1218/SCFlow">SCFlow</a> adds 3D shape constraint into the optimization loop, it formulates the matching process as a pure 2D problem, which is less effective in capturing 3D motions. On the other hand, it cannot work with RGBD images. A common practice is to use <a href="https://doi.org/10.1107/S0567739476001873">RANSAC Kabsch</a> to consume additional depth as a second stage, which however is only locally optimal within each stage.
          <b>(b)</b> SCFlow2 tackles these problems. We introduce an intermediate representation based on 3D scene flow to capture 3D motions in network optimization. Furthermore, we embed depth into the loop by formulating depth as an additional regularization to guide the correlation look-up iteratively, producing an end-to-end trainable system with RGBD images.
        </p>
      </div>
    </div>

  </div>
</section>


<style>
  .video-table {
    width: 100%;
    border-collapse: collapse;
  }
  .video-table td {
    padding: 10px;
    text-align: center !important; 
    vertical-align: middle; /* 垂直居中 */
    border: 1px solid gray;
    
  }
  .video-table video {
    /* width: 100%;
    height: auto; */
    height: 300px;
    width: auto;
  }
  .label-ours {
    color: lime; /* 预定义的鲜艳绿色 */
    font-size: 12px;
    font-weight: bold;
  
  }

  .label-gt {
    color: blue;
    font-size: 12px;
    font-weight: bold;
  }
  a[href="https://rgbinhandscanning.github.io/"],
  a[href="https://chenhsuanlin.bitbucket.io/bundle-adjusting-NeRF/"],
  a[href="https://www.magicleap.com/magic-leap-2"] {
    text-decoration: underline;
  }
</style>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Qualitative results</h2>
        </div>
      </div>
      <div class="content has-text-justified">
        <p>
          </b> Given the same pose initialization as that in GenFlow (<b>"GFlow"</b>) and FoundPose (<b>"FPose"</b>), denoted as <b>"GFlow (init)"</b> and <b>"FPose (init)"</b> respectively, our refinement method (<b>"+ Ours"</b>) produces considerably more accurate results compared to the refinement approaches in their original methods (note how our reprojected 3D mesh aligns better with the object contours) (<span style="color: blue; font-weight: bold;">GT</span>).
        </p>
      </div>
      <div class="hero-body">
        <div class="image-container">
          <img src="./static/images/fig5.png" alt="SCFlow & SCFlow2">
        </div>
      </div>

    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Robustness to initial pose jitter</h2>
        </div>
      </div>
      <div class="content has-text-justified">
        <p>
          We add random rotation and translation noise to GT pose as the initialization and optimize the pose. Our method is robust to inaccurate initialization.
        </p>
      </div>
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/ycbv_53.mp4"
                type="video/mp4">
      </video>
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/ycbv_54.mp4"
                type="video/mp4">
      </video>
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/tless_03.mp4"
                type="video/mp4">
      </video>
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/tless_07.mp4"
                type="video/mp4">
      </video>
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/tless_10.mp4"
                type="video/mp4">
      </video>
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/tudl_01.mp4"
                type="video/mp4">
      </video>
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/tudl_03.mp4"
                type="video/mp4">
      </video>
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/hb_01.mp4"
                type="video/mp4">
      </video>
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/hb_02.mp4"
                type="video/mp4">
      </video>
      <!-- <h2 class="subtitle has-text-centered">
        <span class="dnerf">Nerfies</span> turns selfie videos from your phone into free-viewpointportraits.
      </h2> -->
    </div>
  </div>
</section>


<!-- <section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Robustness to initial pose jitter</h2>
      </div>
    </div>
    <div class="content has-text-justified">
      <p>
        We add random rotation and translation noise to GT pose as the initialization and optimize the pose. Our method is robust to inaccurate initialization.
      </p>
    </div>
<table class="video-table">
  <tr>
    <td style="width: 100px;"><strong>Dataset Name </strong></td>
    <td><strong>Results comparison</strong></td>
  </tr>
  <tr>
    <td><video src="./static/videos/tless_03_jtrefined.mp4" autoplay muted loop playsinline></video></td>
  </tr>
  <tr>
    <td><video src="./static/videos/tless_03_jtrefined_3fps.mp4" autoplay muted loop playsinline></video></td>
  </tr>
  <tr>
    <td><video src="./static/videos/tudl_jtrefined.mp4" autoplay muted loop playsinline></video></td>
  </tr>
  <tr>

    <td><video src="./static/videos/ycbv_jt_refined.mp4" autoplay muted loop playsinline></video></td>
  </tr>
</table>
<div>
</section> -->



<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX: </h2>
    <pre><code>@article{wang2025scflow2,
  author    = {Wang, Qingyuan and Song, Rui and Li, Jiaojiao and Cheng, Kerui and Ferstl, David and Hu, Yinlin},
  title     = {SCFlow2: Plug-and-Play Object Pose Refiner with Shape-Constraint Scene Flow},
  journal   = {CVPR},
  year      = {2025},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
    The webpage is adapted from  <a
    href="https://github.com/nerfies/nerfies.github.io">NeRFies</a>.
  </div>

  </div>
</footer>

</body>
</html>
